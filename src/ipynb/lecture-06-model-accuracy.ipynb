{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rc('figure', figsize=(20, 6))\n",
    "\n",
    "IRIS = load_iris()\n",
    "iris_df = pd.DataFrame(IRIS.data, columns=['sepal_length', 'sepal_width', 'petal_length', 'petal_width'])\n",
    "labels = IRIS.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Bias-Variance Tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In his [1996 paper](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.390.9412&rep=rep1&type=pdf), \"The Lack of A Priori Distinctions between Learning Algorithms,\" David Wolpert asserts that \"There is No Free Lunch in Machine Learning\", essentially saying that the performance of on machine learning models are equivalent when averaged across all possible problems. In other words, there is no way to know which model is going to perform at best for a particular problem. In practice this means that we must have strong methods for assessing the performance of our models.\n",
    "\n",
    "Thinking about model performance is complex. We cannot simply choose the model that performs best with the data that we have. The reason for this is that the data we have represents a sample of the actual data that we could possibly collect now or in the future. The \"best model\" is not necessarily one that performs best on the data that we have. The best model is a model that performs extremely well on the data that we have but it's also capable of generalizing to new data. In statistical learning, we have a framework for thinking about this called The Bias-Variance Tradeoff.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This framework is difficult to understand. Adding to the difficulty is the fact that both \"bias\" and \"variance\" are important concepts for working in applied statistics **and** the meaning of these terms is difficult to reconcile with their meaning when thinking about the Bias-Variance Tradeoff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try to come to a high level understanding of these terms in this setting before looking at them in application. You might think of **bias** as the extent to which a particular model can learn to represent an underlying physical phenomenon. A low bias means that the model was able to learn the phenomenon well. For example, looking at the Iris data set, if we are building a regression model to predict the petal width then bias would be the degree to which our model was able to learn the relationship between petal length and petal width or sepal width and petal width. \n",
    "\n",
    "**Variance** on the other hand is the extent to which a particular model would change if fit with different data. We previously looked at sampling our data set. We saw the measured means of value change with each sample. From this we can infer that a model predicting the meaning with only a few points of data has a high variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Gory Details\n",
    "\n",
    "If we have split our data into a training set and a testing set, then we can think of choosing the best model in terms of optimizing the expected test error, $MSE_{test}$\n",
    "\n",
    "Let's consider sources of possible test error:\n",
    "\n",
    "$$MSE_{test} = \\mathbb{E}\\left[(y-\\widehat{y})^2\\right] = \\text{Var}\\left(\\widehat{y}\\right) + \\left(\\text{Bias}\\left(\\widehat{y}\\right)\\right)^2 + \\text{Var}\\left(\\epsilon\\right)$$\n",
    "\n",
    "This is intended to be a conceptual and not an actual calculation to be performed. Let's think about what each of these terms might represent. The variance is error introduced to the model by the specific choice of training data. Of course this isn't something that we choose, at least not with out using randomness, but the training data that is used will impact the model. By nature, variance is a squared value and that's always positive. Bias is introduced by choosing a specific model. Note that it is squared here and thus also always positive. The last term is the variance caused by noise in the system. We have no way of controlling this, nor of actually knowing what is truly noise and what is model variance or bias. Again this term is always positive.\n",
    "\n",
    "The important thing is that all three of these terms are always positive. The impact of this is that one kind of error cannot be offset by another kind of error. A high variance cannot be offset by a low bias. **In order to choose the best model, we are going to need to simultaneously minimize both bias and variance.** The problem is changing an aspect of our model to decrease one Will typically increase the other -- The Bias-Variance Tradeoff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Assessment\n",
    "\n",
    "Consider these eight models for predicting petal width $x_{pw}$ on the Iris data set, using the other three features, petal length $x_{pl}$, sepal length $x_{sl}$, and sepal width $x_{sw}$\n",
    "\n",
    "\\begin{align*}\n",
    "\\widehat{f}_0 = \\widehat{x}_{pw} &= \\beta_0 \\\\\n",
    "\\widehat{f}_1 = \\widehat{x}_{pw} &= \\beta_0 + \\beta_1x_{pl}\\\\\n",
    "\\widehat{f}_2 = \\widehat{x}_{pw} &= \\beta_0 + \\beta_1x_{sl}\\\\\n",
    "\\widehat{f}_3 = \\widehat{x}_{pw} &= \\beta_0 + \\beta_1x_{sw}\\\\\n",
    "\\widehat{f}_4 = \\widehat{x}_{pw} &= \\beta_0 + \\beta_1x_{pl} + \\beta_1x_{sl}\\\\\n",
    "\\widehat{f}_5 = \\widehat{x}_{pw} &= \\beta_0 + \\beta_1x_{pl} + \\beta_1x_{sw}\\\\\n",
    "\\widehat{f}_6 = \\widehat{x}_{pw} &= \\beta_0 + \\beta_1x_{sl} + \\beta_1x_{sw}\\\\\n",
    "\\widehat{f}_7 = \\widehat{x}_{pw} &= \\beta_0 + \\beta_1x_{pl} + \\beta_1x_{sl}+ \\beta_1x_{sw}\\\\\n",
    "\\end{align*}\n",
    "\n",
    "We will split our dataset into ten randomly chosen subsets of 50 points each. We will assess the performance of each of these models. We will use the mean of the performance to represent bias and the standard deviation of the performance to represent variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = []\n",
    "for _ in range(10):\n",
    "    samples.append(iris_df.sample(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from patsy import dmatrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(actual, predicted):\n",
    "    return sum((actual - predicted)**2)/len(actual)\n",
    "\n",
    "def test_func(model_description):\n",
    "    \n",
    "    test = dict()\n",
    "    \n",
    "    test['samples'] = [dmatrices(model_description, sample) for sample in samples]\n",
    "    test['models']  = [LinearRegression(fit_intercept=False) for _ in range(10)]\n",
    "    test['scores']  = []\n",
    "    \n",
    "    for model, sample in zip(test['models'], test['samples']):\n",
    "        target = sample[0]\n",
    "        features = sample[1]\n",
    "        model.fit(features, target)\n",
    "        test['scores'].append(MSE(model.predict(features), target))\n",
    "    \n",
    "    test['scores'] = np.array(test['scores'])\n",
    "    results = { 'description' : model_description }\n",
    "    results['bias'] = test['scores'].mean()\n",
    "    results['variance'] = test['scores'].std()\n",
    "    return test, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_1, results_1 = test_func(\"petal_width ~ 1\")\n",
    "test_2, results_2 = test_func(\"petal_width ~ 1 + petal_length\")\n",
    "test_3, results_3 = test_func(\"petal_width ~ 1 + sepal_length\")\n",
    "test_4, results_4 = test_func(\"petal_width ~ 1 + sepal_width\")\n",
    "test_5, results_5 = test_func(\"petal_width ~ 1 + petal_length + sepal_width\")\n",
    "test_6, results_6 = test_func(\"petal_width ~ 1 + petal_length + sepal_length\")\n",
    "test_7, results_7 = test_func(\"petal_width ~ 1 + sepal_length + sepal_width\")\n",
    "test_8, results_8 = test_func(\"petal_width ~ 1 + petal_length + sepal_length + sepal_width\")\n",
    "results = [\n",
    "    results_1,\n",
    "    results_2,\n",
    "    results_3,\n",
    "    results_4,\n",
    "    results_5,\n",
    "    results_6,\n",
    "    results_7,\n",
    "    results_8\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(results)\n",
    "results['color'] = ['red','orange', 'yellow','green','blue','cyan','purple' ,'black']\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "for i in results.index:\n",
    "    plt.scatter(results.loc[i].bias, results.loc[i].variance, c=results.loc[i].color, label=results.loc[i].description)\n",
    "plt.xlabel('Bias')\n",
    "plt.ylabel('Variance')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
